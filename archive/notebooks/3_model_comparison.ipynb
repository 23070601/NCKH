{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc918e0",
   "metadata": {},
   "source": [
    "# 3. Model Comparison ‚Äî So s√°nh m√¥ h√¨nh\n",
    "\n",
    "**M·ª•c ti√™u:** So s√°nh nhi·ªÅu m√¥ h√¨nh d·ª± ƒëo√°n volatility ƒë·ªÉ ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t.\n",
    "\n",
    "**Input ch√≠nh:**\n",
    "- data/processed/timestep_*.pt (dataset ƒë√£ x·ª≠ l√Ω)\n",
    "\n",
    "**Output ch√≠nh:**\n",
    "- Bi·ªÉu ƒë·ªì/metrics so s√°nh m√¥ h√¨nh\n",
    "- K·∫øt lu·∫≠n m√¥ h√¨nh t·ªët nh·∫•t\n",
    "\n",
    "**Quy tr√¨nh:**\n",
    "1) Load dataset\n",
    "2) Hu·∫•n luy·ªán nhi·ªÅu m√¥ h√¨nh\n",
    "3) So s√°nh metric & ch·ªçn m√¥ h√¨nh\n",
    "\n",
    "> Ch·∫°y l·∫ßn l∆∞·ª£t c√°c cell ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da54a21",
   "metadata": {},
   "source": [
    "# Model Comparison for Volatility Prediction\n",
    "\n",
    "This notebook compares different algorithms for predicting stock volatility:\n",
    "\n",
    "| Group | Algorithm | Role |\n",
    "|-------|-----------|------|\n",
    "| Baseline | ARIMA | Statistical benchmark |\n",
    "| ML | Random Forest | Non-linear baseline |\n",
    "| DL | LSTM | Temporal modeling |\n",
    "| (Optional) | GRU | Lighter than LSTM |\n",
    "\n",
    "## Evaluation Metrics\n",
    "- **MSE** (Mean Squared Error): Average squared prediction error\n",
    "- **RMSE** (Root Mean Squared Error): Square root of MSE\n",
    "- **MAE** (Mean Absolute Error): Average absolute prediction error\n",
    "- **MAPE** (Mean Absolute Percentage Error): Percentage error\n",
    "- **R¬≤** (Coefficient of Determination): Proportion of variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94634234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Import custom modules\n",
    "from src.datasets import VNStocksDataset\n",
    "from src.datasets.VNStocksDataset import VNStocksVolatilityDataset\n",
    "from src.models import LSTMModel, GRUModel, RandomForestModel, ARIMAModel\n",
    "from src.utils import (\n",
    "    train, calculate_metrics, evaluate_model, evaluate_sklearn_model,\n",
    "    plot_predictions, compare_models, print_metrics_table, count_parameters\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ccca0",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the volatility dataset and create train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = VNStocksVolatilityDataset(\n",
    "    root='../data/',\n",
    "    past_window=25,\n",
    "    future_window=5,\n",
    "    volatility_window=20,\n",
    "    force_reload=False\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Sample shape: {dataset[0].x.shape}\")\n",
    "print(f\"Target shape: {dataset[0].y.shape}\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:]\n",
    "\n",
    "print(f\"\\nTrain size: {len(train_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for DL models\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a6e3a",
   "metadata": {},
   "source": [
    "## 2. Baseline: ARIMA Model\n",
    "\n",
    "Classical statistical model for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0466831",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING ARIMA MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for ARIMA (use first sample as training)\n",
    "# ARIMA trains on each stock's time series independently\n",
    "X_train_arima = train_dataset[0].x  # (nodes, features, timesteps)\n",
    "X_test_arima = test_dataset[0].x\n",
    "y_test_arima = test_dataset[0].y\n",
    "\n",
    "# Train ARIMA\n",
    "arima_model = ARIMAModel(order=(2, 1, 1))\n",
    "print(\"Fitting ARIMA models...\")\n",
    "arima_model.fit(X_train_arima)\n",
    "\n",
    "# Predict volatility\n",
    "print(\"Making predictions...\")\n",
    "y_pred_arima = arima_model.predict_volatility(\n",
    "    X_test_arima,\n",
    "    volatility_window=20,\n",
    "    future_window=5\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_true_arima, y_pred_arima = evaluate_sklearn_model(arima_model, X_test_arima, y_test_arima)\n",
    "arima_metrics = calculate_metrics(y_test_arima.numpy(), y_pred_arima)\n",
    "\n",
    "print(\"\\nARIMA Results:\")\n",
    "for metric, value in arima_metrics.items():\n",
    "    print(f\"  {metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63910c",
   "metadata": {},
   "source": [
    "## 3. ML Model: Random Forest\n",
    "\n",
    "Non-linear model that doesn't explicitly model temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beda8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING RANDOM FOREST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data\n",
    "X_train_rf = torch.stack([data.x for data in train_dataset])\n",
    "y_train_rf = torch.cat([data.y for data in train_dataset])\n",
    "X_test_rf = torch.stack([data.x for data in test_dataset])\n",
    "y_test_rf = torch.cat([data.y for data in test_dataset])\n",
    "\n",
    "print(f\"Train shape: {X_train_rf.shape}, {y_train_rf.shape}\")\n",
    "print(f\"Test shape: {X_test_rf.shape}, {y_test_rf.shape}\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestModel(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nFitting Random Forest...\")\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating...\")\n",
    "y_true_rf, y_pred_rf = evaluate_sklearn_model(rf_model, X_test_rf, y_test_rf)\n",
    "rf_metrics = calculate_metrics(y_true_rf, y_pred_rf)\n",
    "\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "for metric, value in rf_metrics.items():\n",
    "    print(f\"  {metric}: {value:.6f}\")\n",
    "\n",
    "# Feature importance\n",
    "importance = rf_model.feature_importance()\n",
    "print(f\"\\nTop 10 important features (indices): {np.argsort(importance)[-10:][::-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576360d",
   "metadata": {},
   "source": [
    "## 4. Deep Learning: LSTM Model\n",
    "\n",
    "Recurrent neural network for modeling temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING LSTM MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create model\n",
    "in_features = dataset[0].x.shape[1]  # Number of features\n",
    "lstm_model = LSTMModel(\n",
    "    in_features=in_features,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(lstm_model):,}\")\n",
    "\n",
    "# Training configuration\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 50\n",
    "\n",
    "# Train\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "print(\"\\nTraining...\")\n",
    "lstm_history = train(\n",
    "    model=lstm_model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    task_title=\"volatility_LSTM\",\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating...\")\n",
    "y_true_lstm, y_pred_lstm = evaluate_model(lstm_model, test_loader, device)\n",
    "lstm_metrics = calculate_metrics(y_true_lstm, y_pred_lstm)\n",
    "\n",
    "print(\"\\nLSTM Results:\")\n",
    "for metric, value in lstm_metrics.items():\n",
    "    print(f\"  {metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51100f64",
   "metadata": {},
   "source": [
    "## 5. Deep Learning: GRU Model (Optional)\n",
    "\n",
    "Lighter alternative to LSTM with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING GRU MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create model\n",
    "gru_model = GRUModel(\n",
    "    in_features=in_features,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(gru_model):,}\")\n",
    "print(f\"LSTM parameters: {count_parameters(lstm_model):,}\")\n",
    "print(f\"Parameter reduction: {(1 - count_parameters(gru_model)/count_parameters(lstm_model))*100:.1f}%\")\n",
    "\n",
    "# Training configuration\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining...\")\n",
    "gru_history = train(\n",
    "    model=gru_model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    task_title=\"volatility_GRU\",\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating...\")\n",
    "y_true_gru, y_pred_gru = evaluate_model(gru_model, test_loader, device)\n",
    "gru_metrics = calculate_metrics(y_true_gru, y_pred_gru)\n",
    "\n",
    "print(\"\\nGRU Results:\")\n",
    "for metric, value in gru_metrics.items():\n",
    "    print(f\"  {metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9571567",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Compare all models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results = {\n",
    "    'ARIMA': arima_metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'LSTM': lstm_metrics,\n",
    "    'GRU': gru_metrics\n",
    "}\n",
    "\n",
    "# Print comparison table\n",
    "print_metrics_table(results)\n",
    "\n",
    "# Plot comparison\n",
    "fig = compare_models(results)\n",
    "plt.savefig('../data/analysis/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison plot saved to '../data/analysis/model_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results_json = {}\n",
    "for model_name, metrics in results.items():\n",
    "    results_json[model_name] = {k: float(v) for k, v in metrics.items()}\n",
    "\n",
    "with open('../data/analysis/model_comparison_results.json', 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "print(\"Results saved to '../data/analysis/model_comparison_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61dee8",
   "metadata": {},
   "source": [
    "## 7. Visualization of Predictions\n",
    "\n",
    "Visual comparison of model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e503e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for each model\n",
    "models_to_plot = [\n",
    "    ('ARIMA', y_test_arima.numpy(), y_pred_arima),\n",
    "    ('Random Forest', y_true_rf, y_pred_rf),\n",
    "    ('LSTM', y_true_lstm, y_pred_lstm),\n",
    "    ('GRU', y_true_gru, y_pred_gru)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model_name, y_true, y_pred) in enumerate(models_to_plot):\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[i].scatter(y_true_flat[:500], y_pred_flat[:500], alpha=0.5, s=20)\n",
    "    axes[i].plot([y_true_flat.min(), y_true_flat.max()], \n",
    "                [y_true_flat.min(), y_true_flat.max()], 'r--', lw=2)\n",
    "    axes[i].set_xlabel('True Volatility')\n",
    "    axes[i].set_ylabel('Predicted Volatility')\n",
    "    axes[i].set_title(f'{model_name} - Predictions vs True', fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R¬≤ score\n",
    "    r2 = results[model_name]['R2']\n",
    "    axes[i].text(0.05, 0.95, f'R¬≤ = {r2:.4f}', \n",
    "                transform=axes[i].transAxes, \n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/analysis/predictions_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Predictions plot saved to '../data/analysis/predictions_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d934b3c",
   "metadata": {},
   "source": [
    "## 8. Training History (DL Models)\n",
    "\n",
    "Visualize training progress for LSTM and GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c881e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# LSTM history\n",
    "axes[0].plot(lstm_history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(lstm_history['test_loss'], label='Test Loss', linewidth=2)\n",
    "axes[0].axvline(lstm_history['best_epoch'], color='r', linestyle='--', \n",
    "               label=f\"Best Epoch ({lstm_history['best_epoch']})\")\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('LSTM Training History', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# GRU history\n",
    "axes[1].plot(gru_history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(gru_history['test_loss'], label='Test Loss', linewidth=2)\n",
    "axes[1].axvline(gru_history['best_epoch'], color='r', linestyle='--',\n",
    "               label=f\"Best Epoch ({gru_history['best_epoch']})\")\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss (MSE)')\n",
    "axes[1].set_title('GRU Training History', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/analysis/training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved to '../data/analysis/training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf2c76",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "### Best Performing Model\n",
    "\n",
    "Based on the evaluation metrics, we can determine which model performs best for volatility prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676bdb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model by RMSE (lower is better)\n",
    "rmse_scores = {name: metrics['RMSE'] for name, metrics in results.items()}\n",
    "best_model = min(rmse_scores, key=rmse_scores.get)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   RMSE: {rmse_scores[best_model]:.6f}\")\n",
    "print(f\"   R¬≤:   {results[best_model]['R2']:.6f}\")\n",
    "\n",
    "print(\"\\nüìä Model Rankings (by RMSE):\")\n",
    "for i, (model, rmse) in enumerate(sorted(rmse_scores.items(), key=lambda x: x[1]), 1):\n",
    "    print(f\"   {i}. {model:<20} RMSE: {rmse:.6f}\")\n",
    "\n",
    "print(\"\\nüí° Key Findings:\")\n",
    "print(f\"   - DL models (LSTM/GRU) typically outperform classical methods\")\n",
    "print(f\"   - GRU offers similar performance to LSTM with {(1 - count_parameters(gru_model)/count_parameters(lstm_model))*100:.1f}% fewer parameters\")\n",
    "print(f\"   - Random Forest provides strong baseline without temporal modeling\")\n",
    "print(f\"   - ARIMA serves as statistical benchmark\")\n",
    "\n",
    "print(\"\\nüìÅ All results saved to '../data/analysis/' directory\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc80119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'total_samples': len(dataset),\n",
    "        'train_samples': len(train_dataset),\n",
    "        'test_samples': len(test_dataset),\n",
    "        'num_stocks': 98,\n",
    "        'num_features': in_features,\n",
    "        'past_window': 25,\n",
    "        'future_window': 5\n",
    "    },\n",
    "    'models': {\n",
    "        model: {\n",
    "            'metrics': {k: float(v) for k, v in metrics.items()},\n",
    "            'parameters': count_parameters(lstm_model).item() if model == 'LSTM'\n",
    "                         else count_parameters(gru_model).item() if model == 'GRU'\n",
    "                         else 'N/A'\n",
    "        }\n",
    "        for model, metrics in results.items()\n",
    "    },\n",
    "    'best_model': best_model,\n",
    "    'timestamp': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open('../data/analysis/experiment_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Experiment summary saved to '../data/analysis/experiment_summary.json'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
