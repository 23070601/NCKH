# ğŸ¯ Predicting the Volatility and Risk Level of Stock Prices of FDI Enterprises Listed in Vietnam

> **Objective**: Predict the average volatility and risk classification over the next 5 days for 98 FDI stocks listed on the Vietnamese stock market

---

## ğŸš€ CHáº Y Dá»° ÃN (1 Lá»†NH)

```bash
./run_pipeline.sh           # Cached (~8s) hoáº·c láº§n Ä‘áº§u (~400s)
./run_pipeline.sh --force   # Force cháº¡y láº¡i toÃ n bá»™
```

**Pipeline gá»“m 8 bÆ°á»›c**:
1. Thu tháº­p OHLCV + VNIndex
2. Export features ra CSV
3. Build PyTorch tensors
4. Train models (RandomForest, GradientBoosting, Ridge)
5. Generate base predictions
6. Generate improved predictions (vá»›i lag features)
7. Evaluate models
8. Export tables

---

## ğŸ“‚ Cáº¤U TRÃšC Dá»° ÃN

```
NCKH/
â”œâ”€â”€ pipeline/             â†’ 8 bÆ°á»›c pipeline (CORE)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             â†’ values.csv (OHLCV), adj.npy
â”‚   â”œâ”€â”€ features/        â†’ all_features_raw.csv, all_features_processed.csv
â”‚   â”œâ”€â”€ processed/       â†’ timestep_*.pt (723 timesteps)
â”‚   â””â”€â”€ results/         â†’ models, predictions, evaluation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ datasets/        â†’ VNStocksDataset
â”‚   â”œâ”€â”€ models/          â†’ random_forest.py
â”‚   â”œâ”€â”€ utils/           â†’ backtest, inference
â”‚   â”œâ”€â”€ VNStocks.py      â†’ Feature engineering
â”‚   â””â”€â”€ data_utils.py    â†’ Data utilities
â”œâ”€â”€ PROJECT_MAP.md       â†’ ğŸ—ºï¸ Báº¢N Äá»’ CHI TIáº¾T (XEM ÄÃ‚Y)
â””â”€â”€ run_pipeline.sh      â†’ Master script
```

ğŸ‘‰ **Xem [PROJECT_MAP.md](PROJECT_MAP.md) Ä‘á»ƒ hiá»ƒu CHI TIáº¾T tá»«ng file lÃ m gÃ¬**

## ğŸ“Š Dá»® LIá»†U (INPUT/OUTPUT)

### INPUT (X) - 24 features
**File CSV**: [data/features/all_features_raw.csv](data/features/all_features_raw.csv)

**Features gá»“m**:
- OHLCV: Open, High, Low, Close, Volume
- Technical: RSI, MACD, MA_5, MA_10, MA_20, BB_UPPER/MID/LOWER, VOL_20
- Returns: DailyLogReturn, ALR1W, ALR2W, ALR1M, ALR2M
- Market: VNIndex_Close, VNIndex_Return

ğŸ‘‰ Chi tiáº¿t: [DATA_INPUT.md](DATA_INPUT.md)

### OUTPUT (y) - Volatility
**File CSV**: [data/results/exports/all_volatility_labels.csv](data/results/exports/all_volatility_labels.csv)

**Äá»‹nh nghÄ©a**: Volatility = Ä‘á»™ lá»‡ch chuáº©n cá»§a returns trong 20 ngÃ y

**Target**: Dá»± bÃ¡o volatility trung bÃ¬nh **5 ngÃ y tá»›i**

ğŸ‘‰ Chi tiáº¿t: [DATA_OUTPUT.md](DATA_OUTPUT.md)

## Report guide (má»Ÿ file nÃ o khi tháº§y há»i)

### CÃ¢u há»i 1: â€œInput lÃ  gÃ¬? data gá»‘c á»Ÿ Ä‘Ã¢u?â€
- Má»Ÿ: [DATA_INPUT.md](DATA_INPUT.md)
- File dá»¯ liá»‡u: [data/features/all_features_raw.csv](data/features/all_features_raw.csv)

### CÃ¢u há»i 2: â€œOutput lÃ  gÃ¬? label tÃ­nh tháº¿ nÃ o?â€
- Má»Ÿ: [DATA_OUTPUT.md](DATA_OUTPUT.md)
- File output: [data/results/exports/all_volatility_labels.csv](data/results/exports/all_volatility_labels.csv)

### CÃ¢u há»i 3: â€œFeature tÃ­nh á»Ÿ Ä‘Ã¢u?â€
- Má»Ÿ code: [src/VNStocks.py](src/VNStocks.py)

### CÃ¢u há»i 4: â€œDataset .pt táº¡o á»Ÿ Ä‘Ã¢u?â€
- Má»Ÿ code: [src/datasets/VNStocksDataset.py](src/datasets/VNStocksDataset.py)

### CÃ¢u há»i 5: â€œModel train á»Ÿ Ä‘Ã¢u?â€
- Má»Ÿ script: [pipeline/04_train_models.py](pipeline/04_train_models.py)

## Features

âœ… **Smart Caching**: Reuses previously computed results  
âœ… **Force Mode**: `--force` flag triggers full recomputation  
âœ… **Automatic Cleanup**: Removes old files before creating new ones  
âœ… **Fast Re-runs**: Cached pipeline runs in ~8 seconds  
âœ… **Production Ready**: All models saved for deployment

## Project Structure

```
NCKH/
â”œâ”€â”€ run_pipeline.sh                 # Master pipeline runner (recommended)
â”œâ”€â”€ pipeline/                        # Refactored pipeline steps
â”‚   â”œâ”€â”€ 01_collect_values.py
â”‚   â”œâ”€â”€ 02b_export_full_features.py
â”‚   â”œâ”€â”€ 03_build_tensors.py
â”‚   â”œâ”€â”€ 04_train_models.py
â”‚   â”œâ”€â”€ 05_base_predictions.py
â”‚   â”œâ”€â”€ 06_generate_predictions.py
â”‚   â”œâ”€â”€ 07_evaluate.py
â”‚   â”œâ”€â”€ 08_export_tables.py
â”‚   â””â”€â”€ 09_risk_portfolio.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ FINAL_REPORT.md                 # Comprehensive documentation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Raw stock data
â”‚   â”œâ”€â”€ processed/                  # PyTorch datasets (747 timestep files)
â”‚   â”œâ”€â”€ features/                   # Full feature tables (all_features_*.csv)
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ models/                 # Trained models + selectors
â”‚       â”œâ”€â”€ predictions/            # predictions_*.csv
â”‚       â”œâ”€â”€ evaluation/             # metrics + plots
â”‚       â”œâ”€â”€ backtest/               # backtest summaries
â”‚       â””â”€â”€ exports/                # CSV exports for input/output
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ VNStocks.py
    â”œâ”€â”€ data_utils.py
    â”œâ”€â”€ datasets/
    â”œâ”€â”€ models/
    â””â”€â”€ utils/
```

## Core Scripts

### `run_pipeline.sh` (Recommended)
Runs the full pipeline with caching.

**Usage**:
```bash
./run_pipeline.sh
./run_pipeline.sh --force
```

### Pipeline steps
1. **pipeline/01_collect_values.py** â€“ Collect OHLCV + VNIndex â†’ values.csv + adj.npy
2. **pipeline/02b_export_full_features.py** â€“ Export all features â†’ all_features_raw/processed.csv
3. **pipeline/03_build_tensors.py** â€“ Build timestep_*.pt for training
4. **pipeline/04_train_models.py** â€“ Train RF/GB/Ridge/XGBoost + LSTM + classifier
5. **pipeline/05_base_predictions.py** â€“ Base predictions
6. **pipeline/06_generate_predictions.py** â€“ Improved predictions (lag features)
7. **pipeline/07_evaluate.py** â€“ Metrics + plots
8. **pipeline/08_export_tables.py** â€“ Export CSV tables
9. **pipeline/09_risk_portfolio.py** â€“ VaR/CVaR + CVaR portfolio optimization

## Performance & Speed

| Scenario | Time | Details |
|----------|------|---------|
| **Cached Run** | ~8s | All steps use cached results |
| **Full Rerun** | ~150s | Recomputes all steps with --force |
| **Training Only** | ~20s | Model training only |
| **Evaluation Only** | ~10s | Metrics computation only |

### Regression Performance
```
Baseline Model:    RÂ² = -0.015
Improved Ensemble: RÂ² = 0.2416
Improvement:       +2451%
```

### Classification Performance  
```
Baseline:  33.3% accuracy
Improved:  86.42% accuracy
Improvement: +53.1%
```

### Backtest Results (Top-20 Strategy)
```
Sharpe Ratio:      3.467
Return:            +11.43%
Max Drawdown:      -2.79%
```

## Key Features

- **Lag Engineering**: Captures temporal dependencies (t-1, t-2, t-3)
- **Feature Selection**: SelectKBest with f_regression (203 â†’ 15 features)
- **Ensemble Methods**: Combines Ridge, Random Forest, Gradient Boosting
- **Class Balancing**: SMOTE for imbalanced classification
- **Time-Series Split**: 70% train, 15% val, 15% test

## Hyperparameters

**Random Forest**
- n_estimators: 200
- max_depth: 20
- min_samples_split: 5
- max_features: 'sqrt'

**Gradient Boosting**
- n_estimators: 100
- learning_rate: 0.05
- max_depth: 5
- subsample: 0.8

**Ridge Regression**
- alpha: 1.0

**SMOTE**
- k_neighbors: 5

## Dependencies

- scikit-learn (models, feature selection)
- imbalanced-learn (SMOTE)
- pandas, numpy
- torch, torch-geometric
- matplotlib
- yfinance

See requirements.txt for full list.

## Data Structure

**Stock Data**: 98 Vietnamese FDI stocks
**Time Period**: 773 trading days
**Features**: Price, returns, technical indicators (RSI, MACD)
**Graph**: 98Ã—98 correlation matrix

## Workflow

1. Prepare data in data/raw/ and data/processed/
2. Run: `python train_models.py`
3. Run: `python generate_predictions.py`
4. Run: `python evaluate_models.py`
5. Review results in data/results/

## Output Files

- `improved_regressor_*.pkl` - Trained regressor
- `improved_classifier_*.pkl` - Trained classifier
- `feature_selector_*.pkl` - Feature selector
- `predictions_improved_lag_*.csv` - Predictions
- `metrics.json` - Evaluation metrics
- `confusion_matrix.png` - Classification visualization
- `calibration.png` - Prediction calibration

## Notes

- All models use random_state=42 for reproducibility
- Models expect preprocessed torch files in data/processed/
- Feature selection adapts to input data dimensionality
- Time-based train/test split preserves temporal order
- Classification uses volatility percentiles (33.33%, 66.67%)
