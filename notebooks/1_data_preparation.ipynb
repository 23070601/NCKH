{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e91ec73",
   "metadata": {},
   "source": [
    "# Data Preparation for Volatility and Risk Prediction\n",
    "## Predicting Volatility and Risk Level of Stock Prices for FDI Enterprises in Vietnam\n",
    "\n",
    "This notebook prepares stock price data and calculates volatility and risk metrics for Graph Neural Network (GNN) analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8049af",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ac91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For data handling and manipulation\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# For visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For GNN (PyTorch Geometric)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471fa2c",
   "metadata": {},
   "source": [
    "## 2. Load Stock Price Data\n",
    "\n",
    "In this section, we'll load historical stock price data for FDI enterprises in Vietnam. \n",
    "You can use:\n",
    "- CSV files with historical data\n",
    "- APIs (yfinance, VnEX API)\n",
    "- Database connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16662e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create sample data for FDI enterprises in Vietnam\n",
    "# In practice, you would load real data from:\n",
    "# - CSV files: pd.read_csv('path/to/data.csv')\n",
    "# - APIs: yfinance, VnEX\n",
    "# - Databases\n",
    "\n",
    "def generate_sample_stock_data(num_stocks=20, num_days=252*2):\n",
    "    \"\"\"\n",
    "    Generate sample stock price data for demonstration\n",
    "    num_stocks: number of FDI enterprises\n",
    "    num_days: number of trading days (~2 years)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = pd.date_range(end=datetime.now(), periods=num_days, freq='D')\n",
    "    stocks = [f'FDI_Stock_{i:02d}' for i in range(1, num_stocks + 1)]\n",
    "    \n",
    "    data_dict = {'Date': []}\n",
    "    \n",
    "    for stock in stocks:\n",
    "        # Generate realistic stock prices using geometric Brownian motion\n",
    "        np.random.seed(hash(stock) % 2**32)\n",
    "        prices = 100  # Initial price\n",
    "        price_series = [prices]\n",
    "        \n",
    "        for _ in range(num_days - 1):\n",
    "            # Drift and volatility parameters\n",
    "            drift = 0.0001\n",
    "            volatility = 0.02\n",
    "            \n",
    "            # Daily return\n",
    "            daily_return = np.random.normal(drift, volatility)\n",
    "            prices = prices * (1 + daily_return)\n",
    "            price_series.append(prices)\n",
    "        \n",
    "        data_dict[stock] = price_series\n",
    "    \n",
    "    data_dict['Date'] = dates\n",
    "    return pd.DataFrame(data_dict)\n",
    "\n",
    "# Generate sample data\n",
    "stock_data = generate_sample_stock_data(num_stocks=15, num_days=252*2)\n",
    "print(f\"Data shape: {stock_data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(stock_data.head())\n",
    "print(\"\\nData info:\")\n",
    "print(stock_data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7aad7",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba78a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(stock_data.isnull().sum())\n",
    "\n",
    "# Remove duplicates (if any)\n",
    "stock_data = stock_data.drop_duplicates(subset=['Date'], keep='first')\n",
    "\n",
    "# Sort by date\n",
    "stock_data = stock_data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nCleaned data shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data['Date'].min()} to {stock_data['Date'].max()}\")\n",
    "print(f\"Number of trading days: {len(stock_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399909a",
   "metadata": {},
   "source": [
    "## 4. Calculate Returns and Log Returns\n",
    "\n",
    "Returns are fundamental for volatility and risk calculations:\n",
    "- Simple Return: $$R_t = \\frac{P_t - P_{t-1}}{P_{t-1}}$$\n",
    "- Log Return: $$r_t = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80506091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate returns for all stocks\n",
    "price_columns = [col for col in stock_data.columns if col.startswith('FDI_Stock')]\n",
    "\n",
    "# Simple returns\n",
    "returns_data = stock_data[price_columns].pct_change()\n",
    "\n",
    "# Log returns\n",
    "log_returns_data = np.log(stock_data[price_columns] / stock_data[price_columns].shift(1))\n",
    "\n",
    "# Combine with dates\n",
    "returns_data.insert(0, 'Date', stock_data['Date'])\n",
    "log_returns_data.insert(0, 'Date', stock_data['Date'])\n",
    "\n",
    "# Remove first row (NaN values)\n",
    "returns_data = returns_data.dropna()\n",
    "log_returns_data = log_returns_data.dropna()\n",
    "\n",
    "print(\"Simple Returns:\")\n",
    "print(returns_data.head())\n",
    "print(f\"\\nShape: {returns_data.shape}\")\n",
    "\n",
    "print(\"\\n\\nLog Returns:\")\n",
    "print(log_returns_data.head())\n",
    "print(f\"\\nShape: {log_returns_data.shape}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\\nSummary Statistics of Log Returns:\")\n",
    "print(log_returns_data[price_columns].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084e3c9",
   "metadata": {},
   "source": [
    "## 5. Compute Volatility Metrics\n",
    "\n",
    "Volatility measures the dispersion of returns and is crucial for risk assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Historical Volatility (annualized)\n",
    "historical_volatility = log_returns_data[price_columns].std() * np.sqrt(252)\n",
    "\n",
    "print(\"Historical Volatility (Annualized):\")\n",
    "print(historical_volatility)\n",
    "\n",
    "# 2. Rolling Volatility (20-day window)\n",
    "rolling_window = 20\n",
    "rolling_volatility = log_returns_data[price_columns].rolling(window=rolling_window).std() * np.sqrt(252)\n",
    "\n",
    "print(\"\\n\\nRolling Volatility (20-day window) - Last 5 rows:\")\n",
    "print(rolling_volatility.tail())\n",
    "\n",
    "# 3. Exponential Moving Volatility (EWMA)\n",
    "lambda_param = 0.94  # Decay parameter\n",
    "ewma_variance = log_returns_data[price_columns].ewm(span=20, adjust=False).var()\n",
    "exponential_volatility = np.sqrt(ewma_variance) * np.sqrt(252)\n",
    "\n",
    "print(\"\\n\\nExponential Moving Volatility - Last 5 rows:\")\n",
    "print(exponential_volatility.tail())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Historical Volatility\n",
    "historical_volatility.sort_values().plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Historical Volatility (Annualized)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Volatility')\n",
    "\n",
    "# Rolling Volatility - sample stocks\n",
    "rolling_volatility[[price_columns[0], price_columns[1], price_columns[2]]].plot(ax=axes[1])\n",
    "axes[1].set_title('Rolling Volatility (20-day) - Sample Stocks', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Volatility')\n",
    "axes[1].legend()\n",
    "\n",
    "# Exponential Moving Volatility - sample stocks\n",
    "exponential_volatility[[price_columns[0], price_columns[1], price_columns[2]]].plot(ax=axes[2])\n",
    "axes[2].set_title('Exponential Moving Volatility - Sample Stocks', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Volatility')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ebe8d",
   "metadata": {},
   "source": [
    "## 6. Calculate Risk Indicators\n",
    "\n",
    "Key risk metrics for portfolio analysis and GNN features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_indicators = pd.DataFrame(index=price_columns)\n",
    "\n",
    "# 1. Value at Risk (VaR) - 95% confidence level\n",
    "confidence_level = 0.95\n",
    "var_95 = log_returns_data[price_columns].quantile(1 - confidence_level)\n",
    "risk_indicators['VaR_95'] = var_95\n",
    "\n",
    "# 2. Conditional Value at Risk (CVaR) - Expected Shortfall\n",
    "cvar_95 = log_returns_data[price_columns].apply(lambda x: x[x <= x.quantile(1 - confidence_level)].mean())\n",
    "risk_indicators['CVaR_95'] = cvar_95\n",
    "\n",
    "# 3. Sharpe Ratio (assuming risk-free rate = 0)\n",
    "risk_free_rate = 0\n",
    "mean_returns = log_returns_data[price_columns].mean() * 252  # Annualized\n",
    "volatility = log_returns_data[price_columns].std() * np.sqrt(252)  # Annualized\n",
    "risk_indicators['Sharpe_Ratio'] = (mean_returns - risk_free_rate) / volatility\n",
    "\n",
    "# 4. Sortino Ratio (downside risk)\n",
    "downside_returns = log_returns_data[price_columns][log_returns_data[price_columns] < 0]\n",
    "downside_volatility = downside_returns.std() * np.sqrt(252)\n",
    "risk_indicators['Sortino_Ratio'] = (mean_returns - risk_free_rate) / downside_volatility\n",
    "\n",
    "# 5. Maximum Drawdown\n",
    "cumulative_returns = (1 + log_returns_data[price_columns]).cumprod()\n",
    "running_max = cumulative_returns.expanding().max()\n",
    "drawdown = (cumulative_returns - running_max) / running_max\n",
    "max_drawdown = drawdown.min()\n",
    "risk_indicators['Max_Drawdown'] = max_drawdown\n",
    "\n",
    "# 6. Skewness\n",
    "risk_indicators['Skewness'] = log_returns_data[price_columns].skew()\n",
    "\n",
    "# 7. Kurtosis\n",
    "risk_indicators['Kurtosis'] = log_returns_data[price_columns].kurtosis()\n",
    "\n",
    "print(\"Risk Indicators Summary:\")\n",
    "print(risk_indicators)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
    "\n",
    "# VaR 95%\n",
    "risk_indicators['VaR_95'].sort_values().plot(kind='barh', ax=axes[0, 0], color='coral')\n",
    "axes[0, 0].set_title('Value at Risk (95%)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('VaR')\n",
    "\n",
    "# CVaR 95%\n",
    "risk_indicators['CVaR_95'].sort_values().plot(kind='barh', ax=axes[0, 1], color='lightcoral')\n",
    "axes[0, 1].set_title('Conditional Value at Risk (95%)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('CVaR')\n",
    "\n",
    "# Sharpe Ratio\n",
    "risk_indicators['Sharpe_Ratio'].sort_values().plot(kind='barh', ax=axes[0, 2], color='lightgreen')\n",
    "axes[0, 2].set_title('Sharpe Ratio', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Sharpe Ratio')\n",
    "\n",
    "# Maximum Drawdown\n",
    "risk_indicators['Max_Drawdown'].sort_values().plot(kind='barh', ax=axes[1, 0], color='gold')\n",
    "axes[1, 0].set_title('Maximum Drawdown', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Drawdown')\n",
    "\n",
    "# Skewness\n",
    "risk_indicators['Skewness'].sort_values().plot(kind='barh', ax=axes[1, 1], color='lightblue')\n",
    "axes[1, 1].set_title('Skewness', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Skewness')\n",
    "\n",
    "# Kurtosis\n",
    "risk_indicators['Kurtosis'].sort_values().plot(kind='barh', ax=axes[1, 2], color='plum')\n",
    "axes[1, 2].set_title('Kurtosis (Excess)', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Kurtosis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668b768",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering for GNN Input\n",
    "\n",
    "Create feature matrices for Graph Neural Networks where:\n",
    "- **Nodes** represent individual stocks\n",
    "- **Node Features** include price movements, volatility, and risk metrics\n",
    "- **Edges** represent correlations or sector relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bb53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node features for each stock\n",
    "num_stocks = len(price_columns)\n",
    "\n",
    "# Initialize feature matrix (num_stocks x num_features)\n",
    "node_features_list = []\n",
    "\n",
    "for stock in price_columns:\n",
    "    stock_returns = log_returns_data[stock].values\n",
    "    \n",
    "    # Feature 1: Mean return (annualized)\n",
    "    mean_return = stock_returns.mean() * 252\n",
    "    \n",
    "    # Feature 2: Historical volatility (annualized)\n",
    "    hist_vol = stock_returns.std() * np.sqrt(252)\n",
    "    \n",
    "    # Feature 3: Skewness\n",
    "    skewness = stats.skew(stock_returns)\n",
    "    \n",
    "    # Feature 4: Kurtosis\n",
    "    kurtosis = stats.kurtosis(stock_returns)\n",
    "    \n",
    "    # Feature 5: Max daily return\n",
    "    max_return = stock_returns.max()\n",
    "    \n",
    "    # Feature 6: Min daily return\n",
    "    min_return = stock_returns.min()\n",
    "    \n",
    "    # Feature 7: VaR (95%)\n",
    "    var_95_val = np.percentile(stock_returns, 5)\n",
    "    \n",
    "    # Feature 8: Average positive return\n",
    "    pos_returns = stock_returns[stock_returns > 0]\n",
    "    avg_pos_return = pos_returns.mean() if len(pos_returns) > 0 else 0\n",
    "    \n",
    "    node_features_list.append([\n",
    "        mean_return, hist_vol, skewness, kurtosis, \n",
    "        max_return, min_return, var_95_val, avg_pos_return\n",
    "    ])\n",
    "\n",
    "node_features = np.array(node_features_list)\n",
    "print(f\"Node Features shape: {node_features.shape}\")\n",
    "print(f\"Features per node: {node_features.shape[1]}\")\n",
    "print(\"\\nNode Features Matrix (first 3 stocks):\")\n",
    "print(node_features[:3])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "node_features_normalized = scaler.fit_transform(node_features)\n",
    "\n",
    "print(\"\\nNormalized Node Features (first 3 stocks):\")\n",
    "print(node_features_normalized[:3])\n",
    "\n",
    "# Create correlation matrix for edge construction\n",
    "correlation_matrix = log_returns_data[price_columns].corr()\n",
    "\n",
    "print(f\"\\nCorrelation Matrix shape: {correlation_matrix.shape}\")\n",
    "print(\"Correlation Matrix (first 3x3):\")\n",
    "print(correlation_matrix.iloc[:3, :3])\n",
    "\n",
    "# Visualization of correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, square=True, \n",
    "            xticklabels=False, yticklabels=False, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Stock Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07a52e",
   "metadata": {},
   "source": [
    "## 8. Prepare Data for Model Training\n",
    "\n",
    "Create graph structures and prepare train/test splits for GNN models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e620b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge indices based on correlation threshold\n",
    "correlation_threshold = 0.3\n",
    "edge_indices = []\n",
    "edge_weights = []\n",
    "\n",
    "for i in range(num_stocks):\n",
    "    for j in range(i + 1, num_stocks):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > correlation_threshold:\n",
    "            # Create undirected edges\n",
    "            edge_indices.append([i, j])\n",
    "            edge_indices.append([j, i])\n",
    "            edge_weights.append(corr_value)\n",
    "            edge_weights.append(corr_value)\n",
    "\n",
    "edge_indices = np.array(edge_indices).T if edge_indices else np.array([[], []])\n",
    "edge_weights = np.array(edge_weights)\n",
    "\n",
    "print(f\"Number of edges: {len(edge_weights)}\")\n",
    "print(f\"Edge indices shape: {edge_indices.shape}\")\n",
    "print(f\"Edge weights shape: {edge_weights.shape}\")\n",
    "\n",
    "# Create PyTorch Geometric Data object\n",
    "x = torch.tensor(node_features_normalized, dtype=torch.float32)\n",
    "edge_index = torch.tensor(edge_indices, dtype=torch.long)\n",
    "edge_attr = torch.tensor(edge_weights, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# Target: Volatility of each stock for prediction task\n",
    "y = torch.tensor(historical_volatility.values, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# Create the graph data object\n",
    "graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n",
    "print(f\"\\nGraph Data Object:\")\n",
    "print(f\"  Number of nodes: {graph_data.num_nodes}\")\n",
    "print(f\"  Number of edges: {graph_data.num_edges}\")\n",
    "print(f\"  Node feature dimension: {graph_data.num_node_features}\")\n",
    "print(f\"  Edge attribute dimension: {graph_data.num_edge_features}\")\n",
    "print(f\"  Target dimension: {graph_data.y.shape}\")\n",
    "\n",
    "# Train/Test split on temporal data\n",
    "# We'll use time-series split: train on earlier data, test on later data\n",
    "train_ratio = 0.8\n",
    "split_idx = int(len(log_returns_data) * train_ratio)\n",
    "\n",
    "print(f\"\\nTemporal Split:\")\n",
    "print(f\"  Training data: {log_returns_data['Date'].iloc[0].date()} to {log_returns_data['Date'].iloc[split_idx].date()}\")\n",
    "print(f\"  Testing data: {log_returns_data['Date'].iloc[split_idx].date()} to {log_returns_data['Date'].iloc[-1].date()}\")\n",
    "\n",
    "# Save prepared data\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "save_dir = '/Users/hoc/Documents/NCKH/code/data'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save all relevant data\n",
    "data_dict = {\n",
    "    'stock_data': stock_data,\n",
    "    'returns_data': returns_data,\n",
    "    'log_returns_data': log_returns_data,\n",
    "    'node_features': node_features,\n",
    "    'node_features_normalized': node_features_normalized,\n",
    "    'correlation_matrix': correlation_matrix,\n",
    "    'risk_indicators': risk_indicators,\n",
    "    'graph_data': graph_data,\n",
    "    'historical_volatility': historical_volatility,\n",
    "    'price_columns': price_columns,\n",
    "    'split_idx': split_idx\n",
    "}\n",
    "\n",
    "with open(f'{save_dir}/prepared_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f)\n",
    "\n",
    "print(f\"\\nâœ“ Data saved to {save_dir}/prepared_data.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
