{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508dd6a1",
   "metadata": {},
   "source": [
    "# Data Preparation for Volatility Prediction\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Loading the PyTorch Geometric dataset\n",
    "2. Exploring temporal graph structure\n",
    "3. Analyzing feature distributions\n",
    "4. Visualizing stock relationships and volatility patterns\n",
    "5. Preparing train/test splits for model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb91529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from src.datasets import VNStocksDataset\n",
    "from src.datasets.VNStocksDataset import VNStocksVolatilityDataset\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36974e28",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n",
    "\n",
    "First, let's load and examine the raw data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw values\n",
    "values = pd.read_csv('../data/values.csv').set_index(['Symbol', 'Date'])\n",
    "print(f\"Dataset shape: {values.shape}\")\n",
    "print(f\"\\nFeatures: {list(values.columns)}\")\n",
    "print(f\"\\nNumber of stocks: {len(values.index.get_level_values('Symbol').unique())}\")\n",
    "print(f\"Number of trading days: {len(values.index.get_level_values('Date').unique())}\")\n",
    "\n",
    "values.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adjacency matrix\n",
    "adj = np.load('../data/adj.npy')\n",
    "print(f\"Adjacency matrix shape: {adj.shape}\")\n",
    "print(f\"Number of edges: {np.count_nonzero(adj)}\")\n",
    "print(f\"Graph density: {np.count_nonzero(adj) / (adj.shape[0] * adj.shape[1]):.4f}\")\n",
    "\n",
    "# Visualize adjacency matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(adj, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Stock Correlation Adjacency Matrix')\n",
    "plt.xlabel('Stock Index')\n",
    "plt.ylabel('Stock Index')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157e2f7",
   "metadata": {},
   "source": [
    "## 2. Create PyTorch Geometric Dataset\n",
    "\n",
    "Now let's create temporal graph datasets using our custom dataset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1aa98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standard dataset (for price prediction baseline)\n",
    "past_window = 25  # 5 weeks of trading days\n",
    "future_window = 1  # Predict next day\n",
    "\n",
    "dataset = VNStocksDataset(\n",
    "    root='../data/',\n",
    "    past_window=past_window,\n",
    "    future_window=future_window,\n",
    "    force_reload=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} temporal snapshots\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "sample = dataset[0]\n",
    "print(f\"  x shape: {sample.x.shape}  # (nodes, features, timesteps)\")\n",
    "print(f\"  y shape: {sample.y.shape}  # (nodes, future_window)\")\n",
    "print(f\"  edge_index shape: {sample.edge_index.shape}\")\n",
    "print(f\"  edge_weight shape: {sample.edge_weight.shape}\")\n",
    "print(f\"  close_price shape: {sample.close_price.shape}\")\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create volatility dataset (main task)\n",
    "volatility_dataset = VNStocksVolatilityDataset(\n",
    "    root='../data/',\n",
    "    past_window=25,\n",
    "    future_window=5,  # Predict volatility over next 5 days\n",
    "    volatility_window=20,  # Calculate volatility using 20-day window\n",
    "    force_reload=True\n",
    ")\n",
    "\n",
    "print(f\"Volatility dataset size: {len(volatility_dataset)} temporal snapshots\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "vol_sample = volatility_dataset[0]\n",
    "print(f\"  x shape: {vol_sample.x.shape}\")\n",
    "print(f\"  y shape (volatility): {vol_sample.y.shape}\")\n",
    "print(f\"  volatility shape: {vol_sample.volatility.shape}\")\n",
    "\n",
    "vol_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe329265",
   "metadata": {},
   "source": [
    "## 3. Feature Analysis\n",
    "\n",
    "Analyze the distribution and statistics of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b306877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature statistics\n",
    "feature_names = ['NormClose', 'DailyLogReturn', 'ALR1W', 'ALR2W', 'ALR1M', 'ALR2M', 'RSI', 'MACD']\n",
    "\n",
    "print(\"Feature Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "for i, feat_name in enumerate(feature_names):\n",
    "    feat_values = values.iloc[:, i+1].values  # +1 to skip 'Close'\n",
    "    print(f\"\\n{feat_name}:\")\n",
    "    print(f\"  Mean: {feat_values.mean():.6f}\")\n",
    "    print(f\"  Std:  {feat_values.std():.6f}\")\n",
    "    print(f\"  Min:  {feat_values.min():.6f}\")\n",
    "    print(f\"  Max:  {feat_values.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df52876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feat_name in enumerate(feature_names):\n",
    "    feat_values = values.iloc[:, i+1].values\n",
    "    axes[i].hist(feat_values, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'{feat_name} Distribution')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ba29a",
   "metadata": {},
   "source": [
    "## 4. Volatility Analysis\n",
    "\n",
    "Analyze volatility patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfadb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate volatility for all stocks\n",
    "stocks = values.index.get_level_values('Symbol').unique()\n",
    "volatilities = []\n",
    "\n",
    "for stock in stocks:\n",
    "    stock_data = values.loc[stock]\n",
    "    returns = np.diff(np.log(stock_data['Close'].values))\n",
    "    volatility = pd.Series(returns).rolling(window=20).std().values\n",
    "    volatilities.append(volatility)\n",
    "\n",
    "volatilities = np.array(volatilities)\n",
    "print(f\"Volatility matrix shape: {volatilities.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot volatility over time for random stocks\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "np.random.seed(42)\n",
    "random_stocks = np.random.choice(len(stocks), 4, replace=False)\n",
    "\n",
    "dates = pd.to_datetime(values.loc[stocks[0]].index)\n",
    "\n",
    "for i, stock_idx in enumerate(random_stocks):\n",
    "    axes[i].plot(dates, volatilities[stock_idx], linewidth=1.5)\n",
    "    axes[i].set_title(f'{stocks[stock_idx]} - Volatility Over Time')\n",
    "    axes[i].set_xlabel('Date')\n",
    "    axes[i].set_ylabel('Volatility (20-day rolling std)')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e1dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average volatility across all stocks\n",
    "avg_volatility = np.nanmean(volatilities, axis=0)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(dates, avg_volatility, linewidth=2, color='darkblue')\n",
    "plt.fill_between(dates, avg_volatility, alpha=0.3)\n",
    "plt.title('Average Market Volatility (All Stocks)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage volatility statistics:\")\n",
    "print(f\"  Mean: {np.nanmean(avg_volatility):.6f}\")\n",
    "print(f\"  Std:  {np.nanstd(avg_volatility):.6f}\")\n",
    "print(f\"  Max:  {np.nanmax(avg_volatility):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef1175",
   "metadata": {},
   "source": [
    "## 5. Graph Structure Analysis\n",
    "\n",
    "Analyze the correlation-based graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae337d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree distribution\n",
    "degrees = adj.sum(axis=1) > 0  # Binary: has connections or not\n",
    "out_degrees = (adj > 0).sum(axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Degree distribution histogram\n",
    "axes[0].hist(out_degrees, bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Node Degree Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Degree (Number of Connections)')\n",
    "axes[0].set_ylabel('Number of Nodes')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Edge weight distribution\n",
    "edge_weights = adj[adj > 0]\n",
    "axes[1].hist(edge_weights, bins=30, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[1].set_title('Edge Weight Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Correlation Value')\n",
    "axes[1].set_ylabel('Number of Edges')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Graph statistics:\")\n",
    "print(f\"  Number of nodes: {adj.shape[0]}\")\n",
    "print(f\"  Number of edges: {np.count_nonzero(adj)}\")\n",
    "print(f\"  Average degree: {out_degrees.mean():.2f}\")\n",
    "print(f\"  Max degree: {out_degrees.max()}\")\n",
    "print(f\"  Average edge weight: {edge_weights.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d6422",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split\n",
    "\n",
    "Prepare data splits for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard 80-20 train-test split for time series\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(volatility_dataset) * train_ratio)\n",
    "\n",
    "train_dataset = volatility_dataset[:train_size]\n",
    "test_dataset = volatility_dataset[train_size:]\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Total samples: {len(volatility_dataset)}\")\n",
    "print(f\"  Train samples: {len(train_dataset)} ({len(train_dataset)/len(volatility_dataset)*100:.1f}%)\")\n",
    "print(f\"  Test samples:  {len(test_dataset)} ({len(test_dataset)/len(volatility_dataset)*100:.1f}%)\")\n",
    "\n",
    "# Save split indices for reproducibility\n",
    "split_info = {\n",
    "    'train_size': train_size,\n",
    "    'test_size': len(volatility_dataset) - train_size,\n",
    "    'train_ratio': train_ratio,\n",
    "    'past_window': past_window,\n",
    "    'future_window': future_window\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/train_test_split.json', 'w') as f:\n",
    "    json.dump(split_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nSplit configuration saved to '../data/train_test_split.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batch processing\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch:\")\n",
    "print(f\"  x shape: {batch.x.shape}\")\n",
    "print(f\"  y shape: {batch.y.shape}\")\n",
    "print(f\"  batch size: {batch.num_graphs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8e496",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Data preparation is complete! We have:\n",
    "\n",
    "### Dataset\n",
    "- **98 Vietnamese FDI stocks** from 2022-01-01 to 2024-12-31\n",
    "- **773 trading days** of historical data\n",
    "- **9 features** per stock: Close, NormClose, DailyLogReturn, ALR1W, ALR2W, ALR1M, ALR2M, RSI, MACD\n",
    "\n",
    "### Graph Structure\n",
    "- **52 edges** based on correlation threshold (0.1)\n",
    "- **Correlation-based adjacency matrix** capturing stock relationships\n",
    "- **Sparse graph** with density of 0.54%\n",
    "\n",
    "### Temporal Snapshots\n",
    "- **Input window**: 25 trading days (~5 weeks)\n",
    "- **Prediction window**: 5 trading days (volatility over next week)\n",
    "- **Volatility calculation**: 20-day rolling standard deviation\n",
    "\n",
    "### Splits\n",
    "- **Train**: 80% of temporal snapshots\n",
    "- **Test**: 20% of temporal snapshots\n",
    "\n",
    "### Next Steps\n",
    "1. Implement baseline models (ARIMA)\n",
    "2. Implement ML models (Random Forest)\n",
    "3. Implement DL models (LSTM, GRU)\n",
    "4. Compare model performance on volatility prediction task"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
