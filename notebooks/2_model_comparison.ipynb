{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a002546",
   "metadata": {},
   "source": [
    "# Model Comparison Framework\n",
    "## Comparing Algorithms for FDI Stock Volatility Prediction\n",
    "\n",
    "This notebook compares multiple forecasting algorithms across different complexity levels:\n",
    "\n",
    "| Nhóm     | Thuật toán              | Vai trò           |\n",
    "|----------|-------------------------|-------------------|\n",
    "| Baseline | Historical Mean / ARIMA | Mốc so sánh        |\n",
    "| ML       | Random Forest           | Phi tuyến          |\n",
    "| DL       | LSTM                    | Quan hệ thời gian  |\n",
    "| DL (opt) | GRU                     | So sánh nội bộ     |\n",
    "\n",
    "**Goal**: Test each algorithm and recommend the optimal one based on performance metrics and research needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744df8f5",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our model comparison framework\n",
    "from model_comparison import (\n",
    "    ModelComparator, \n",
    "    HistoricalMeanModel, \n",
    "    ARIMAModel,\n",
    "    RandomForestVolatilityModel,\n",
    "    LSTMVolatilityModel,\n",
    "    GRUVolatilityModel,\n",
    "    create_sequences,\n",
    "    split_data\n",
    ")\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01ea10",
   "metadata": {},
   "source": [
    "## Section 2: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load volatility data from prepared data\n",
    "# Assuming you've run 1_data_preparation.ipynb and generated volatility metrics\n",
    "\n",
    "# For now, we'll use sample data. Replace with your actual data:\n",
    "data_path = '../data/values.csv'  # Use actual data after preparation\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    print(f\"✓ Data loaded: {data.shape}\")\n",
    "else:\n",
    "    print(\"⚠ Data file not found. Using sample data.\")\n",
    "    # Generate sample volatility data for demonstration\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start='2022-01-01', periods=250, freq='D')\n",
    "    # Simulate stock price\n",
    "    price = 100 + np.cumsum(np.random.randn(250) * 2)\n",
    "    # Calculate rolling volatility (20-day window)\n",
    "    returns = np.log(price[1:] / price[:-1])\n",
    "    volatility = pd.Series(returns).rolling(window=20).std() * np.sqrt(252)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'volatility': volatility.values\n",
    "    })\n",
    "    \n",
    "    print(f\"✓ Sample data generated: {data.shape}\")\n",
    "\n",
    "print(f\"\\nData Info:\")\n",
    "print(f\"  Shape: {data.shape}\")\n",
    "print(f\"  Columns: {data.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(data.head())\n",
    "print(f\"\\nStatistics:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419acc22",
   "metadata": {},
   "source": [
    "### Visualize Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract volatility values\n",
    "if 'volatility' in data.columns:\n",
    "    volatility_data = data['volatility'].dropna().values\n",
    "else:\n",
    "    # Use first numeric column\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    volatility_data = data[numeric_cols[0]].dropna().values\n",
    "\n",
    "# Plot time series\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(volatility_data, linewidth=1.5)\n",
    "plt.title('Stock Volatility Time Series', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Time (days)')\n",
    "plt.ylabel('Volatility')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(volatility_data, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.title('Volatility Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Volatility Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data Statistics:\")\n",
    "print(f\"  Mean: {np.mean(volatility_data):.6f}\")\n",
    "print(f\"  Std:  {np.std(volatility_data):.6f}\")\n",
    "print(f\"  Min:  {np.min(volatility_data):.6f}\")\n",
    "print(f\"  Max:  {np.max(volatility_data):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9b0f4",
   "metadata": {},
   "source": [
    "## Section 3: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN values\n",
    "volatility_clean = volatility_data[~np.isnan(volatility_data)]\n",
    "\n",
    "print(f\"Clean data shape: {volatility_clean.shape}\")\n",
    "print(f\"Removed NaN values: {len(volatility_data) - len(volatility_clean)}\")\n",
    "\n",
    "# Normalize data to [0, 1] range\n",
    "scaler = StandardScaler()\n",
    "volatility_normalized = scaler.fit_transform(volatility_clean.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"\\nNormalized data:\")\n",
    "print(f\"  Mean: {np.mean(volatility_normalized):.6f}\")\n",
    "print(f\"  Std:  {np.std(volatility_normalized):.6f}\")\n",
    "\n",
    "# Create lag features for ML models\n",
    "def create_lag_features(data, lags=[1, 5, 10, 20]):\n",
    "    \"\"\"Create lag features for ML models\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['target'] = data\n",
    "    \n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = data.shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df['rolling_mean_5'] = pd.Series(data).rolling(5).mean()\n",
    "    df['rolling_std_5'] = pd.Series(data).rolling(5).std()\n",
    "    df['rolling_mean_10'] = pd.Series(data).rolling(10).mean()\n",
    "    df['rolling_std_10'] = pd.Series(data).rolling(10).std()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "ml_features = create_lag_features(volatility_normalized)\n",
    "print(f\"\\nML Features shape: {ml_features.shape}\")\n",
    "print(f\"Features: {ml_features.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26ace4",
   "metadata": {},
   "source": [
    "## Section 4: Train-Test Split for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split ratio: 60% train, 20% val, 20% test\n",
    "lookback = 20\n",
    "\n",
    "# For RNN models: create sequences\n",
    "X_rnn, y_rnn = create_sequences(volatility_normalized, lookback)\n",
    "(X_train_rnn, y_train_rnn), (X_val_rnn, y_val_rnn), (X_test_rnn, y_test_rnn) = split_data(X_rnn, y_rnn)\n",
    "\n",
    "print(f\"RNN Sequences:\")\n",
    "print(f\"  X_train: {X_train_rnn.shape}\")\n",
    "print(f\"  X_val:   {X_val_rnn.shape}\")\n",
    "print(f\"  X_test:  {X_test_rnn.shape}\")\n",
    "\n",
    "# For ML models: use lag features\n",
    "X_ml = ml_features.drop('target', axis=1).values\n",
    "y_ml = ml_features['target'].values\n",
    "\n",
    "n = len(X_ml)\n",
    "train_idx = int(n * 0.6)\n",
    "val_idx = int(n * 0.8)\n",
    "\n",
    "X_train_ml = X_ml[:train_idx]\n",
    "y_train_ml = y_ml[:train_idx]\n",
    "X_test_ml = X_ml[val_idx:]\n",
    "y_test_ml = y_ml[val_idx:]\n",
    "\n",
    "print(f\"\\nML Features:\")\n",
    "print(f\"  X_train: {X_train_ml.shape}\")\n",
    "print(f\"  X_test:  {X_test_ml.shape}\")\n",
    "\n",
    "# For Baseline models: use full time series\n",
    "train_baseline = volatility_normalized[:train_idx]\n",
    "test_baseline = volatility_normalized[val_idx:]\n",
    "\n",
    "print(f\"\\nBaseline Data:\")\n",
    "print(f\"  Train: {len(train_baseline)}\")\n",
    "print(f\"  Test:  {len(test_baseline)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194f3d48",
   "metadata": {},
   "source": [
    "## Section 5: Baseline Models - Historical Mean and ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0947e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BASELINE: Historical Mean\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Historical Mean Model\n",
    "hm_model = HistoricalMeanModel(window=20)\n",
    "hm_model.fit(train_baseline)\n",
    "hm_pred = hm_model.predict(test_baseline)\n",
    "\n",
    "# Denormalize predictions\n",
    "hm_pred_denorm = scaler.inverse_transform(hm_pred.reshape(-1, 1)).flatten()\n",
    "test_denorm = scaler.inverse_transform(test_baseline.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "hm_rmse = np.sqrt(mean_squared_error(test_denorm, hm_pred_denorm))\n",
    "hm_mae = mean_absolute_error(test_denorm, hm_pred_denorm)\n",
    "hm_mape = mean_absolute_percentage_error(test_denorm, hm_pred_denorm)\n",
    "\n",
    "print(f\"\\nHistorical Mean Results:\")\n",
    "print(f\"  RMSE: {hm_rmse:.6f}\")\n",
    "print(f\"  MAE:  {hm_mae:.6f}\")\n",
    "print(f\"  MAPE: {hm_mape:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23392a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BASELINE: ARIMA Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    \n",
    "    # Fit ARIMA model (1,1,1) - standard for financial data\n",
    "    print(\"\\nFitting ARIMA(1,1,1)...\")\n",
    "    arima_model = ARIMA(train_baseline, order=(1, 1, 1))\n",
    "    arima_fit = arima_model.fit()\n",
    "    \n",
    "    # Forecast\n",
    "    arima_forecast = arima_fit.get_forecast(steps=len(test_baseline))\n",
    "    arima_pred = arima_forecast.predicted_mean.values\n",
    "    \n",
    "    # Denormalize\n",
    "    arima_pred_denorm = scaler.inverse_transform(arima_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    arima_rmse = np.sqrt(mean_squared_error(test_denorm, arima_pred_denorm))\n",
    "    arima_mae = mean_absolute_error(test_denorm, arima_pred_denorm)\n",
    "    arima_mape = mean_absolute_percentage_error(test_denorm, arima_pred_denorm)\n",
    "    \n",
    "    print(f\"\\nARIMA(1,1,1) Results:\")\n",
    "    print(f\"  RMSE: {arima_rmse:.6f}\")\n",
    "    print(f\"  MAE:  {arima_mae:.6f}\")\n",
    "    print(f\"  MAPE: {arima_mape:.6f}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ statsmodels not installed. Install with: pip install statsmodels\")\n",
    "    print(\"  Skipping ARIMA model.\")\n",
    "    arima_rmse = np.inf\n",
    "    arima_mae = np.inf\n",
    "    arima_mape = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f71c7f",
   "metadata": {},
   "source": [
    "## Section 6: Random Forest for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ML MODEL: Random Forest\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fit Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_ml, y_train_ml)\n",
    "\n",
    "# Predict\n",
    "rf_pred = rf_model.predict(X_test_ml)\n",
    "\n",
    "# Denormalize\n",
    "rf_pred_denorm = scaler.inverse_transform(rf_pred.reshape(-1, 1)).flatten()\n",
    "test_ml_denorm = scaler.inverse_transform(y_test_ml.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "rf_rmse = np.sqrt(mean_squared_error(test_ml_denorm, rf_pred_denorm))\n",
    "rf_mae = mean_absolute_error(test_ml_denorm, rf_pred_denorm)\n",
    "rf_mape = mean_absolute_percentage_error(test_ml_denorm, rf_pred_denorm)\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"  RMSE: {rf_rmse:.6f}\")\n",
    "print(f\"  MAE:  {rf_mae:.6f}\")\n",
    "print(f\"  MAPE: {rf_mape:.6f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_names = ml_features.drop('target', axis=1).columns\n",
    "importances = rf_model.feature_importances_\n",
    "print(f\"\\nTop 5 Important Features:\")\n",
    "for i, (name, importance) in enumerate(sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)[:5]):\n",
    "    print(f\"  {i+1}. {name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ffe24b",
   "metadata": {},
   "source": [
    "## Section 7: LSTM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DL MODEL: LSTM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Reshape for LSTM (samples, timesteps, features)\n",
    "    X_train_lstm = X_train_rnn.reshape((X_train_rnn.shape[0], X_train_rnn.shape[1], 1))\n",
    "    X_test_lstm = X_test_rnn.reshape((X_test_rnn.shape[0], X_test_rnn.shape[1], 1))\n",
    "    \n",
    "    print(f\"\\nTraining LSTM...\")\n",
    "    lstm_model = LSTMVolatilityModel(lookback=lookback, lstm_units=50, epochs=50, batch_size=16)\n",
    "    lstm_model.fit(X_train_lstm, y_train_rnn)\n",
    "    \n",
    "    # Predict\n",
    "    lstm_pred = lstm_model.predict(X_test_lstm).flatten()\n",
    "    \n",
    "    # Denormalize\n",
    "    lstm_pred_denorm = scaler.inverse_transform(lstm_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_lstm_denorm = scaler.inverse_transform(y_test_rnn.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    lstm_rmse = np.sqrt(mean_squared_error(y_test_lstm_denorm, lstm_pred_denorm))\n",
    "    lstm_mae = mean_absolute_error(y_test_lstm_denorm, lstm_pred_denorm)\n",
    "    lstm_mape = mean_absolute_percentage_error(y_test_lstm_denorm, lstm_pred_denorm)\n",
    "    \n",
    "    print(f\"\\nLSTM Results:\")\n",
    "    print(f\"  RMSE: {lstm_rmse:.6f}\")\n",
    "    print(f\"  MAE:  {lstm_mae:.6f}\")\n",
    "    print(f\"  MAPE: {lstm_mape:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ LSTM training error: {e}\")\n",
    "    print(\"  Install TensorFlow: pip install tensorflow\")\n",
    "    lstm_rmse = np.inf\n",
    "    lstm_mae = np.inf\n",
    "    lstm_mape = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93efd6",
   "metadata": {},
   "source": [
    "## Section 8: GRU Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DL MODEL: GRU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Reshape for GRU (samples, timesteps, features)\n",
    "    X_train_gru = X_train_rnn.reshape((X_train_rnn.shape[0], X_train_rnn.shape[1], 1))\n",
    "    X_test_gru = X_test_rnn.reshape((X_test_rnn.shape[0], X_test_rnn.shape[1], 1))\n",
    "    \n",
    "    print(f\"\\nTraining GRU...\")\n",
    "    gru_model = GRUVolatilityModel(lookback=lookback, gru_units=50, epochs=50, batch_size=16)\n",
    "    gru_model.fit(X_train_gru, y_train_rnn)\n",
    "    \n",
    "    # Predict\n",
    "    gru_pred = gru_model.predict(X_test_gru).flatten()\n",
    "    \n",
    "    # Denormalize\n",
    "    gru_pred_denorm = scaler.inverse_transform(gru_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_gru_denorm = scaler.inverse_transform(y_test_rnn.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    gru_rmse = np.sqrt(mean_squared_error(y_test_gru_denorm, gru_pred_denorm))\n",
    "    gru_mae = mean_absolute_error(y_test_gru_denorm, gru_pred_denorm)\n",
    "    gru_mape = mean_absolute_percentage_error(y_test_gru_denorm, gru_pred_denorm)\n",
    "    \n",
    "    print(f\"\\nGRU Results:\")\n",
    "    print(f\"  RMSE: {gru_rmse:.6f}\")\n",
    "    print(f\"  MAE:  {gru_mae:.6f}\")\n",
    "    print(f\"  MAPE: {gru_mape:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ GRU training error: {e}\")\n",
    "    print(\"  Install TensorFlow: pip install tensorflow\")\n",
    "    gru_rmse = np.inf\n",
    "    gru_mae = np.inf\n",
    "    gru_mape = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304396d1",
   "metadata": {},
   "source": [
    "## Section 9: Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results_data = {\n",
    "    'Model': ['Historical Mean', 'ARIMA(1,1,1)', 'Random Forest', 'LSTM', 'GRU'],\n",
    "    'Group': ['Baseline', 'Baseline', 'ML', 'DL', 'DL'],\n",
    "    'RMSE': [hm_rmse, arima_rmse, rf_rmse, lstm_rmse, gru_rmse],\n",
    "    'MAE': [hm_mae, arima_mae, rf_mae, lstm_mae, gru_mae],\n",
    "    'MAPE': [hm_mape, arima_mape, rf_mape, lstm_mape, gru_mape]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df = results_df.sort_values('RMSE').reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate improvement\n",
    "best_rmse = results_df['RMSE'].min()\n",
    "baseline_rmse = results_df[results_df['Group'] == 'Baseline']['RMSE'].min()\n",
    "improvement = ((baseline_rmse - best_rmse) / baseline_rmse) * 100\n",
    "\n",
    "print(f\"\\nImprovement over best baseline: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed728cf1",
   "metadata": {},
   "source": [
    "### Detailed Comparison by Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b28315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RMSE Comparison\n",
    "axes[0].barh(results_df['Model'], results_df['RMSE'], color=['red' if g == 'Baseline' else 'blue' if g == 'ML' else 'green' \n",
    "                                                                for g in results_df['Group']])\n",
    "axes[0].set_xlabel('RMSE (Lower is Better)', fontweight='bold')\n",
    "axes[0].set_title('Root Mean Squared Error', fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# MAE Comparison\n",
    "axes[1].barh(results_df['Model'], results_df['MAE'], color=['red' if g == 'Baseline' else 'blue' if g == 'ML' else 'green' \n",
    "                                                               for g in results_df['Group']])\n",
    "axes[1].set_xlabel('MAE (Lower is Better)', fontweight='bold')\n",
    "axes[1].set_title('Mean Absolute Error', fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# MAPE Comparison\n",
    "axes[2].barh(results_df['Model'], results_df['MAPE'], color=['red' if g == 'Baseline' else 'blue' if g == 'ML' else 'green' \n",
    "                                                                for g in results_df['Group']])\n",
    "axes[2].set_xlabel('MAPE % (Lower is Better)', fontweight='bold')\n",
    "axes[2].set_title('Mean Absolute Percentage Error', fontweight='bold')\n",
    "axes[2].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Legend:\")\n",
    "print(\"  Red   = Baseline Models\")\n",
    "print(\"  Blue  = ML Models\")\n",
    "print(\"  Green = DL Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b7f79",
   "metadata": {},
   "source": [
    "## Section 10: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55203735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align all predictions for visualization\n",
    "# Note: Different models use different test sets, so we need to adjust\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Historical Mean\n",
    "axes[0, 0].plot(test_denorm, 'o-', label='Actual', linewidth=2, markersize=4)\n",
    "axes[0, 0].plot(hm_pred_denorm, 's--', label='Predicted', linewidth=2, markersize=4)\n",
    "axes[0, 0].set_title(f'Historical Mean (RMSE: {hm_rmse:.4f})', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Volatility')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ARIMA\n",
    "if arima_rmse != np.inf:\n",
    "    axes[0, 1].plot(test_denorm, 'o-', label='Actual', linewidth=2, markersize=4)\n",
    "    axes[0, 1].plot(arima_pred_denorm, 's--', label='Predicted', linewidth=2, markersize=4)\n",
    "    axes[0, 1].set_title(f'ARIMA(1,1,1) (RMSE: {arima_rmse:.4f})', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Volatility')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest\n",
    "axes[0, 2].plot(test_ml_denorm, 'o-', label='Actual', linewidth=2, markersize=4)\n",
    "axes[0, 2].plot(rf_pred_denorm, 's--', label='Predicted', linewidth=2, markersize=4)\n",
    "axes[0, 2].set_title(f'Random Forest (RMSE: {rf_rmse:.4f})', fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Volatility')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# LSTM\n",
    "if lstm_rmse != np.inf:\n",
    "    axes[1, 0].plot(y_test_lstm_denorm, 'o-', label='Actual', linewidth=2, markersize=4)\n",
    "    axes[1, 0].plot(lstm_pred_denorm, 's--', label='Predicted', linewidth=2, markersize=4)\n",
    "    axes[1, 0].set_title(f'LSTM (RMSE: {lstm_rmse:.4f})', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Volatility')\n",
    "    axes[1, 0].set_xlabel('Time')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# GRU\n",
    "if gru_rmse != np.inf:\n",
    "    axes[1, 1].plot(y_test_gru_denorm, 'o-', label='Actual', linewidth=2, markersize=4)\n",
    "    axes[1, 1].plot(gru_pred_denorm, 's--', label='Predicted', linewidth=2, markersize=4)\n",
    "    axes[1, 1].set_title(f'GRU (RMSE: {gru_rmse:.4f})', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Volatility')\n",
    "    axes[1, 1].set_xlabel('Time')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "axes[1, 2].text(0.1, 0.9, \"Model Comparison Summary\\n\", transform=axes[1, 2].transAxes, \n",
    "                fontsize=12, fontweight='bold', verticalalignment='top')\n",
    "summary_text = \"\\n\".join([f\"{row['Model']}: RMSE={row['RMSE']:.4f}\" \n",
    "                          for _, row in results_df.iterrows()])\n",
    "axes[1, 2].text(0.1, 0.75, summary_text, transform=axes[1, 2].transAxes, \n",
    "                fontsize=10, verticalalignment='top', family='monospace')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19bc16f",
   "metadata": {},
   "source": [
    "## Section 11: Select Optimal Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMAL ALGORITHM RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best model by RMSE\n",
    "best_model_idx = results_df['RMSE'].idxmin()\n",
    "best_model = results_df.loc[best_model_idx]\n",
    "\n",
    "print(f\"\\n✓ OPTIMAL ALGORITHM: {best_model['Model']} ({best_model['Group']})\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  RMSE: {best_model['RMSE']:.6f}\")\n",
    "print(f\"  MAE:  {best_model['MAE']:.6f}\")\n",
    "print(f\"  MAPE: {best_model['MAPE']:.6f}%\")\n",
    "\n",
    "# Analysis\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_best = results_df[results_df['Group'] == 'Baseline'].iloc[0]\n",
    "ml_best = results_df[results_df['Group'] == 'ML'].iloc[0]\n",
    "dl_best = results_df[results_df['Group'] == 'DL'].iloc[0]\n",
    "\n",
    "print(f\"\\n1. BASELINE (Reference Point):\")\n",
    "print(f\"   Best: {baseline_best['Model']}\")\n",
    "print(f\"   RMSE: {baseline_best['RMSE']:.6f}\")\n",
    "print(f\"   Role: Provides benchmark for comparison\")\n",
    "\n",
    "print(f\"\\n2. MACHINE LEARNING:\")\n",
    "print(f\"   Best: {ml_best['Model']}\")\n",
    "print(f\"   RMSE: {ml_best['RMSE']:.6f}\")\n",
    "print(f\"   Improvement: {((baseline_best['RMSE'] - ml_best['RMSE'])/baseline_best['RMSE']*100):.2f}% over baseline\")\n",
    "print(f\"   Role: Captures non-linear patterns\")\n",
    "\n",
    "print(f\"\\n3. DEEP LEARNING:\")\n",
    "print(f\"   Best: {dl_best['Model']}\")\n",
    "print(f\"   RMSE: {dl_best['RMSE']:.6f}\")\n",
    "print(f\"   Improvement: {((baseline_best['RMSE'] - dl_best['RMSE'])/baseline_best['RMSE']*100):.2f}% over baseline\")\n",
    "print(f\"   Role: Captures temporal dependencies\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"DECISION FRAMEWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nChoose {best_model['Model']} because:\")\n",
    "print(f\"  ✓ Lowest RMSE among all models\")\n",
    "print(f\"  ✓ Group: {best_model['Group']} (balance between complexity and accuracy)\")\n",
    "\n",
    "if best_model['Group'] == 'Baseline':\n",
    "    print(f\"  ✓ Simple, interpretable, fast to train\")\n",
    "    print(f\"  ✓ Low computational requirements\")\n",
    "elif best_model['Group'] == 'ML':\n",
    "    print(f\"  ✓ Captures non-linear relationships\")\n",
    "    print(f\"  ✓ Feature importance available\")\n",
    "    print(f\"  ✓ Fast training and inference\")\n",
    "else:\n",
    "    print(f\"  ✓ Captures long-term temporal dependencies\")\n",
    "    print(f\"  ✓ Suitable for sequence modeling\")\n",
    "    print(f\"  ✓ Can incorporate multiple features\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1. Use {best_model['Model']} as primary model\")\n",
    "print(f\"2. Fine-tune hyperparameters for better performance\")\n",
    "print(f\"3. Validate on larger dataset (full 100 stocks)\")\n",
    "print(f\"4. Compare with GNN-based approach (optional enhancement)\")\n",
    "print(f\"5. Analyze FDI-specific patterns in predictions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
